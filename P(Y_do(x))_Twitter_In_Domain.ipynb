{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ngsgLM40FRPP",
        "outputId": "3fe2523f-f35d-4b3a-f752-11f139e5711d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D3BJWvP72t0i"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaModel\n",
        "import pandas as pd\n",
        "import random\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from collections import Counter\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EyKMT_4xScbz"
      },
      "outputs": [],
      "source": [
        "# Load FLAN-T5\n",
        "model_name = \"google/flan-t5-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
        "model.eval().to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define candidate labels INCLUDING 'others'\n",
        "candidate_labels = [\"race\", \"religion\", \"gender\", \"sexual orientation\", \"politics\", \"immigration\", \"others\"]\n",
        "\n",
        "# === Generator ===\n",
        "def generate_topics(\n",
        "    text,\n",
        "    num_return_sequences=20,\n",
        "    top_k=50,\n",
        "    top_p=0.95,\n",
        "    temperature=1.4,\n",
        "    max_length=20\n",
        "):\n",
        "    prompt = (\n",
        "        \"Choose the most relevant topic for the following text. \"\n",
        "        \"Available topics: race, religion, gender, sexual orientation, politics, immigration, others.\\n\"\n",
        "        \"Use 'others' if none of the specific categories clearly apply.\\n\\n\"\n",
        "\n",
        "        \"Text: 'The president signed a new healthcare bill.'\\nTopic: politics\\n\"\n",
        "        \"Text: 'She attends church every Sunday.'\\nTopic: religion\\n\"\n",
        "        \"Text: 'He identifies as non-binary and uses they/them pronouns.'\\nTopic: gender\\n\"\n",
        "        \"Text: 'They were denied entry due to their visa status.'\\nTopic: immigration\\n\"\n",
        "        \"Text: 'I love painting and long walks in the forest.'\\nTopic: others\\n\"\n",
        "        f\"Text: {text}\\nTopic:\"\n",
        "    )\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    outputs = model.generate(\n",
        "        input_ids=input_ids,\n",
        "        do_sample=True,\n",
        "        top_k=top_k,\n",
        "        top_p=top_p,\n",
        "        temperature=temperature,\n",
        "        num_return_sequences=num_return_sequences,\n",
        "        max_length=max_length\n",
        "    )\n",
        "\n",
        "    decoded = [tokenizer.decode(out, skip_special_tokens=True).strip().lower() for out in outputs]\n",
        "\n",
        "    # Map decoded output to closest valid label\n",
        "    def map_to_label(output):\n",
        "        for label in candidate_labels:\n",
        "            if label in output:\n",
        "                return label\n",
        "        return \"others\"  # Still fallback for off-target generations\n",
        "\n",
        "    mapped_labels = [map_to_label(output) for output in decoded]\n",
        "    return mapped_labels\n",
        "\n",
        "# === Distribution utility ===\n",
        "def generate_topic_distribution(text, num_return_sequences=30):\n",
        "    samples = generate_topics(text, num_return_sequences=num_return_sequences)\n",
        "    counts = Counter(samples)\n",
        "    total = sum(counts.values())\n",
        "    return {label: counts.get(label, 0) / total for label in candidate_labels}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6F4lc5POALU"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0N7UtfY5YdMl"
      },
      "outputs": [],
      "source": [
        "def estimate_p_star_a(\n",
        "    csv_path,\n",
        "    text_column='text',\n",
        "    num_samples_per_text=30,\n",
        "    candidate_labels=None,\n",
        "    output_path=None,\n",
        "    verbose=True\n",
        "):\n",
        "    if candidate_labels is None:\n",
        "        candidate_labels = [\"race\", \"religion\", \"gender\", \"sexual orientation\", \"politics\", \"immigration\", \"others\"]\n",
        "\n",
        "    # Load text data\n",
        "    df = pd.read_csv(csv_path)\n",
        "    texts = df[text_column].tolist()\n",
        "\n",
        "    # Aggregate topic distributions\n",
        "    total_counter = Counter()\n",
        "\n",
        "    for i, text in enumerate(texts):\n",
        "        dist = generate_topic_distribution(text, num_return_sequences=num_samples_per_text)\n",
        "        total_counter.update(dist)\n",
        "        if verbose and i % 100 == 0:\n",
        "            print(f\"Processed {i}/{len(texts)} rows...\")\n",
        "\n",
        "    # Normalize\n",
        "    total = sum(total_counter.values())\n",
        "    p_star_a = {label: total_counter.get(label, 0) / total for label in candidate_labels}\n",
        "\n",
        "    # Save or return\n",
        "    p_star_df = pd.DataFrame.from_dict(p_star_a, orient='index', columns=['P*(a)']).reset_index()\n",
        "    p_star_df = p_star_df.rename(columns={'index': 'topic'})\n",
        "\n",
        "    if output_path:\n",
        "        p_star_df.to_csv(output_path, index=False)\n",
        "        if verbose:\n",
        "            print(f\"Saved to {output_path}\")\n",
        "\n",
        "    return p_star_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Rvzd4KHF28Ec",
        "outputId": "8f5f0e06-1e7f-4b05-f89a-4183d035929b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 0/15475 rows...\n",
            "Processed 100/15475 rows...\n",
            "Processed 200/15475 rows...\n",
            "Processed 300/15475 rows...\n",
            "Processed 400/15475 rows...\n",
            "Processed 500/15475 rows...\n",
            "Processed 600/15475 rows...\n",
            "Processed 700/15475 rows...\n",
            "Processed 800/15475 rows...\n",
            "Processed 900/15475 rows...\n",
            "Processed 1000/15475 rows...\n",
            "Processed 1100/15475 rows...\n",
            "Processed 1200/15475 rows...\n",
            "Processed 1300/15475 rows...\n",
            "Processed 1400/15475 rows...\n",
            "Processed 1500/15475 rows...\n",
            "Processed 1600/15475 rows...\n",
            "Processed 1700/15475 rows...\n",
            "Processed 1800/15475 rows...\n",
            "Processed 1900/15475 rows...\n",
            "Processed 2000/15475 rows...\n",
            "Processed 2100/15475 rows...\n",
            "Processed 2200/15475 rows...\n",
            "Processed 2300/15475 rows...\n",
            "Processed 2400/15475 rows...\n",
            "Processed 2500/15475 rows...\n",
            "Processed 2600/15475 rows...\n",
            "Processed 2700/15475 rows...\n",
            "Processed 2800/15475 rows...\n",
            "Processed 2900/15475 rows...\n",
            "Processed 3000/15475 rows...\n",
            "Processed 3100/15475 rows...\n",
            "Processed 3200/15475 rows...\n",
            "Processed 3300/15475 rows...\n",
            "Processed 3400/15475 rows...\n",
            "Processed 3500/15475 rows...\n",
            "Processed 3600/15475 rows...\n",
            "Processed 3700/15475 rows...\n",
            "Processed 3800/15475 rows...\n",
            "Processed 3900/15475 rows...\n",
            "Processed 4000/15475 rows...\n",
            "Processed 4100/15475 rows...\n",
            "Processed 4200/15475 rows...\n",
            "Processed 4300/15475 rows...\n",
            "Processed 4400/15475 rows...\n",
            "Processed 4500/15475 rows...\n",
            "Processed 4600/15475 rows...\n",
            "Processed 4700/15475 rows...\n",
            "Processed 4800/15475 rows...\n",
            "Processed 4900/15475 rows...\n",
            "Processed 5000/15475 rows...\n",
            "Processed 5100/15475 rows...\n",
            "Processed 5200/15475 rows...\n",
            "Processed 5300/15475 rows...\n",
            "Processed 5400/15475 rows...\n",
            "Processed 5500/15475 rows...\n",
            "Processed 5600/15475 rows...\n",
            "Processed 5700/15475 rows...\n",
            "Processed 5800/15475 rows...\n",
            "Processed 5900/15475 rows...\n",
            "Processed 6000/15475 rows...\n"
          ]
        }
      ],
      "source": [
        "p_star_df = estimate_p_star_a(\n",
        "    csv_path='/content/drive/MyDrive/test_twitter.csv',\n",
        "    num_samples_per_text=30,\n",
        "    output_path='/content/drive/MyDrive/p_star_a_test_twitter_distribution.csv'\n",
        ")\n",
        "print(p_star_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcNtW8Aa_MTd",
        "outputId": "e3ce032d-e52a-4981-fdee-dabffea1fc94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 15475/15475 [16:24<00:00, 15.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transported model accuracy: 0.9574\n",
            "Predictions saved to /content/drive/MyDrive/test_twitter_causal_predictions.csv\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import pandas as pd\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load test set and P*(a)\n",
        "df = pd.read_csv('/content/drive/MyDrive/train.csv')\n",
        "p_star_df = pd.read_csv('/content/drive/MyDrive/p_star_a_test_twitter_distribution.csv')\n",
        "p_star = dict(zip(p_star_df['topic'], p_star_df['P*(a)']))\n",
        "\n",
        "# Load conditional model P(Y | x, a)\n",
        "model_path = \"/content/drive/MyDrive/hate_model_conditional\"\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model.eval()\n",
        "\n",
        "# Same candidate label list used in training\n",
        "candidate_labels = list(p_star.keys())\n",
        "\n",
        "# Compute P(Y | x, a) * P*(a) over topic samples\n",
        "def compute_hate_probability(text, p_star, model, tokenizer, device):\n",
        "    weighted_probs = []\n",
        "    for a_label, p_a in p_star.items():\n",
        "        # Construct input: \"[TOPIC: a_label] text\"\n",
        "        input_text = f\"[TOPIC: {a_label}] {text}\"\n",
        "        encoded = tokenizer(input_text, return_tensors=\"pt\", truncation=True, padding='max_length', max_length=128).to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(**encoded)\n",
        "            prob = torch.softmax(output.logits, dim=1).squeeze()  # shape: [2]\n",
        "            weighted = prob * p_a\n",
        "            weighted_probs.append(weighted.cpu().numpy())\n",
        "\n",
        "    total_prob = np.sum(weighted_probs, axis=0)  # shape: [2]\n",
        "    return total_prob  # [P(not hate), P(hate)]\n",
        "\n",
        "# Predict and collect results\n",
        "results = []\n",
        "all_preds = []\n",
        "all_labels = []\n",
        "\n",
        "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
        "    text = row['text']\n",
        "    true_label = row['hate_label']\n",
        "\n",
        "    probs = compute_hate_probability(text, p_star, model, tokenizer, device)\n",
        "    prob_hate = float(probs[1])\n",
        "    pred_label = int(prob_hate >= 0.5)\n",
        "\n",
        "    all_preds.append(pred_label)\n",
        "    all_labels.append(true_label)\n",
        "\n",
        "    results.append({\n",
        "        \"text\": text,\n",
        "        \"true_label\": true_label,\n",
        "        \"predicted_label\": pred_label,\n",
        "        \"prob_hate\": prob_hate\n",
        "    })\n",
        "\n",
        "# Compute accuracy\n",
        "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
        "print(f\"Transported model accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Save to CSV\n",
        "results_df = pd.DataFrame(results)\n",
        "results_df.to_csv('/content/drive/MyDrive/test_twitter_causal_predictions.csv', index=False)\n",
        "print(\"Predictions saved to /content/drive/MyDrive/test_twitter_causal_predictions.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OD2rwdgDHBG"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}